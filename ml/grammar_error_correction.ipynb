{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920cfbd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/archit/miniconda3/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/archit/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/archit/miniconda3/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/archit/miniconda3/lib/python3.10/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/archit/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /Users/archit/miniconda3/lib/python3.10/site-packages (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7588ae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                            | 0/10 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|█▎                                                                                               | 70/5000 [00:01<01:23, 59.23it/s]\u001b[A\n",
      " 15%|██████████████▍                                                                                | 757/5000 [00:01<00:08, 491.99it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:02<00:00, 2258.84it/s]\u001b[A\n",
      " 10%|██████████                                                                                          | 1/10 [00:12<01:50, 12.29s/it]\n",
      "  0%|                                                                                                          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|███████████▏                                                                                   | 588/5000 [00:05<00:40, 110.13it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:05<00:00, 892.34it/s]\u001b[A\n",
      " 20%|████████████████████                                                                                | 2/10 [00:43<03:07, 23.50s/it]\n",
      "  0%|                                                                                                          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:03<00:00, 1392.91it/s]\u001b[A\n",
      " 30%|██████████████████████████████                                                                      | 3/10 [01:13<03:06, 26.63s/it]\n",
      "  0%|                                                                                                          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|████████████▍                                                                                  | 653/5000 [00:04<00:31, 136.18it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:05<00:00, 985.93it/s]\u001b[A\n",
      " 40%|████████████████████████████████████████                                                            | 4/10 [01:45<02:50, 28.47s/it]\n",
      "  0%|                                                                                                          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:03<00:00, 1416.83it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████████████████                                                  | 5/10 [02:19<02:32, 30.57s/it]\n",
      "  0%|                                                                                                          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|████████████▍                                                                                  | 653/5000 [00:02<00:18, 239.68it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:03<00:00, 1658.56it/s]\u001b[A\n",
      " 60%|████████████████████████████████████████████████████████████                                        | 6/10 [02:52<02:06, 31.53s/it]\n",
      "  0%|                                                                                                          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:03<00:00, 1453.99it/s]\u001b[A\n",
      " 70%|██████████████████████████████████████████████████████████████████████                              | 7/10 [03:27<01:37, 32.55s/it]\n",
      "  0%|                                                                                                          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:03<00:00, 1511.21it/s]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████████████████████████████                    | 8/10 [04:00<01:05, 32.55s/it]\n",
      "  0%|                                                                                                          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      " 13%|████████████▍                                                                                  | 653/5000 [00:04<00:27, 157.14it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:04<00:00, 1124.84it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [04:31<00:32, 32.28s/it]\n",
      "  0%|                                                                                                          | 0/5000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:04<00:00, 1012.54it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [05:05<00:00, 30.59s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "with open(\"gec_data_truncated.csv\", \"w\") as f:\n",
    "    write = csv.writer(f, escapechar=\"\\\\\")\n",
    "    write.writerow([\"in\", \"out\"])\n",
    "    for i in tqdm(range(10)):\n",
    "        with open(f\"./gec_data/C4_200M.tsv-0000{i}-of-00010\") as d:\n",
    "            inp_out_pairs = d.readlines()\n",
    "            d_data = [list(map(str.strip, inp_out_pairs[j].split(\"\\t\"))) for j in tqdm(range(5_000))]\n",
    "            write.writerows(d_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9d3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gec_df = pd.read_csv('gec_data_truncated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478128c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in</th>\n",
       "      <th>out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcoin is for $7,094 this morning, which Coin...</td>\n",
       "      <td>Bitcoin goes for $7,094 this morning, accordin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The effect of widespread dud targets two face ...</td>\n",
       "      <td>1. The effect of \"widespread dud\" targets two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tax on sales of stores for non residents are s...</td>\n",
       "      <td>Capital Gains tax on the sale of properties fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much many brands and sellers still in the market.</td>\n",
       "      <td>Many brands and sellers still in the market.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is is the latest Maintenance release of S...</td>\n",
       "      <td>This is is the latest maintenance release of S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  in  \\\n",
       "0  Bitcoin is for $7,094 this morning, which Coin...   \n",
       "1  The effect of widespread dud targets two face ...   \n",
       "2  tax on sales of stores for non residents are s...   \n",
       "3  Much many brands and sellers still in the market.   \n",
       "4  this is is the latest Maintenance release of S...   \n",
       "\n",
       "                                                 out  \n",
       "0  Bitcoin goes for $7,094 this morning, accordin...  \n",
       "1  1. The effect of \"widespread dud\" targets two ...  \n",
       "2  Capital Gains tax on the sale of properties fo...  \n",
       "3       Many brands and sellers still in the market.  \n",
       "4  This is is the latest maintenance release of S...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bea423b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/archit/miniconda3/lib/python3.10/site-packages (0.1.97)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archit/miniconda3/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "\n",
    "model_name = 't5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "token_model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553afb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(gec_df, test_size=0.20, shuffle=True)\n",
    "validation_df, test_df = train_test_split(test_df, test_size=0.50, shuffle=True)\n",
    "validation_df.to_csv(\"validation_data.csv\", index=False)\n",
    "test_df.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73e7bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>33.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.028451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>484.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token_len\n",
       "count  5000.000000\n",
       "mean     33.505800\n",
       "std      26.028451\n",
       "min       6.000000\n",
       "25%      17.000000\n",
       "50%      27.000000\n",
       "75%      42.000000\n",
       "max     484.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df['token_len'] = validation_df['in'].apply(lambda inp: len(tokenizer(inp).input_ids))\n",
    "validation_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8392ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96d5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer,print_text=False):         \n",
    "        self.dataset = dataset\n",
    "        self.pad_to_max_length = False\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        self.max_len = 128\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def tokenize_data(self, in_out_pair):\n",
    "        input_, target_ = in_out_pair['in'], in_out_pair['out']\n",
    "\n",
    "        tokenized_inputs = self.tokenizer(input_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True,\n",
    "                                            truncation=True)\n",
    "    \n",
    "        tokenized_targets = self.tokenizer(target_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True,\n",
    "                                            truncation=True)\n",
    "\n",
    "        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n",
    "            \"attention_mask\": tokenized_inputs['attention_mask'],\n",
    "            \"labels\": tokenized_targets['input_ids']\n",
    "        }\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "        \n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, len(inputs[k]))\n",
    "\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "810739c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 43\n",
      "attention_mask 43\n",
      "labels 45\n",
      "{'input_ids': [37, 682, 28, 16009, 9952, 1195, 38, 3, 9, 1253, 56, 817, 1937, 19, 24, 34, 22, 7, 66, 396, 514, 12, 24460, 1737, 135, 38, 6, 497, 6, 8929, 16023, 6, 38, 25, 653, 12, 1344, 3, 9, 12803, 8109, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [332, 3, 88, 682, 28, 16009, 9952, 1195, 38, 3, 9, 1253, 12, 817, 1937, 19, 24, 34, 22, 7, 66, 396, 514, 12, 24460, 1737, 135, 38, 6, 497, 6, 8929, 16023, 6, 38, 25, 653, 12, 1344, 3, 9, 12803, 8109, 5, 1]}\n"
     ]
    }
   ],
   "source": [
    "train_gec_data = GrammarDataset(train_dataset, tokenizer, True)\n",
    "print(train_gec_data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f52697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=token_model, padding='longest', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "966ca887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "batch_size = 32\n",
    "args = Seq2SeqTrainingArguments(output_dir=\"./gec_out\",\n",
    "                        evaluation_strategy=\"epoch\",\n",
    "                        per_device_train_batch_size=batch_size,\n",
    "                        per_device_eval_batch_size=batch_size,\n",
    "                        learning_rate=1e-5,\n",
    "                        num_train_epochs=1,\n",
    "                        weight_decay=0.01,\n",
    "                        predict_with_generate=True,\n",
    "                        save_steps = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a99c70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in /Users/archit/miniconda3/lib/python3.10/site-packages (0.1.2)\r\n",
      "Requirement already satisfied: numpy in /Users/archit/miniconda3/lib/python3.10/site-packages (from rouge_score) (1.24.1)\r\n",
      "Requirement already satisfied: absl-py in /Users/archit/miniconda3/lib/python3.10/site-packages (from rouge_score) (1.3.0)\r\n",
      "Requirement already satisfied: nltk in /Users/archit/miniconda3/lib/python3.10/site-packages (from rouge_score) (3.8.1)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/archit/miniconda3/lib/python3.10/site-packages (from rouge_score) (1.15.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/archit/miniconda3/lib/python3.10/site-packages (from nltk->rouge_score) (2022.10.31)\r\n",
      "Requirement already satisfied: tqdm in /Users/archit/miniconda3/lib/python3.10/site-packages (from nltk->rouge_score) (4.64.1)\r\n",
      "Requirement already satisfied: joblib in /Users/archit/miniconda3/lib/python3.10/site-packages (from nltk->rouge_score) (1.2.0)\r\n",
      "Requirement already satisfied: click in /Users/archit/miniconda3/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.3)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_s/t0dxy3t91bn2pn1_261pdm680000gn/T/ipykernel_72312/3414290936.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score\n",
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def eval_metrics(in_out_pairs):\n",
    "    preds, labels = in_out_pairs\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    rouge_data = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in rouge_data.items()}\n",
    "    \n",
    "    pred_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(pred_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf9b1318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archit/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1266' max='1266' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1266/1266 3:47:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.593096</td>\n",
       "      <td>70.131500</td>\n",
       "      <td>59.714600</td>\n",
       "      <td>69.344100</td>\n",
       "      <td>69.388900</td>\n",
       "      <td>17.369200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1266, training_loss=0.718926319767137, metrics={'train_runtime': 13667.3974, 'train_samples_per_second': 2.963, 'train_steps_per_second': 0.093, 'total_flos': 4761165812244480.0, 'train_loss': 0.718926319767137, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "gec_model = Seq2SeqTrainer(model=token_model, \n",
    "                args=args, \n",
    "                train_dataset=GrammarDataset(train_dataset, tokenizer),\n",
    "                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=eval_metrics)\n",
    "\n",
    "gec_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "720ee4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gec_model.save_model(\"./gec_model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06744396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, T5Tokenizer\n",
    "gec_tokenizer = T5Tokenizer.from_pretrained(\"./gec_model_final\")\n",
    "gec_model = AutoModelForSeq2SeqLM.from_pretrained(\"./gec_model_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e39acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archit/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 20])\n",
      "Orig: She ain't gonna goes to the store.\n",
      "Corr: She ain't gonna go to the store.\n",
      "\n",
      "Orig: They has been playing soccer all day.\n",
      "Corr: They have been playing soccer all day.\n",
      "\n",
      "Orig: I goed to the movies last night.\n",
      "Corr: I went to the movies last night.\n",
      "\n",
      "Orig: He don't likes it.\n",
      "Corr: He doesn't like it.\n",
      "\n",
      "Orig: He don't visits us.\n",
      "Corr: He doesn't visit us.\n",
      "\n",
      "Orig: We was watching TV when the power went out.\n",
      "Corr: We were watching TV when the power went out.\n",
      "\n",
      "Orig: The dog chases the cat up the tree.\n",
      "Corr: The dog chases the cat up the tree.\n",
      "\n",
      "Orig: Me and him is best friends.\n",
      "Corr: Me and him are best friends.\n",
      "\n",
      "Orig: Oh, I'm goin' to the city for visit my friend. How 'bout you?\n",
      "Corr: Oh, I'm going to the city to visit my friend. How'bout you\n",
      "\n",
      "Orig: I meet my friend for weekend. Family is nice, yes?\n",
      "Corr: I meet my friend for the weekend. Family is nice, yes?\n",
      "\n",
      "Orig: The affect of the tsunami is incomprehensible\n",
      "Corr: The impact of the tsunami is incomprehensible.\n",
      "\n",
      "Orig: This cinnamom powder cut thru the grease.\n",
      "Corr: This cinnamon powder cuts through the grease.\n",
      "\n",
      "Orig: I uh, likes pizza and also soda.\n",
      "Corr: I uh, like pizza and soda.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\n",
    "    \"She ain't gonna goes to the store.\", \n",
    "    \"They has been playing soccer all day.\", \n",
    "    \"I goed to the movies last night.\",\n",
    "    \"He don't likes it.\",\n",
    "    \"He don't visits us.\",\n",
    "    \"We was watching TV when the power went out.\",\n",
    "    \"The dog chases the cat up the tree.\",\n",
    "    \"Me and him is best friends.\",\n",
    "    \"Oh, I'm goin' to the city for visit my friend. How 'bout you?\",\n",
    "    \"I meet my friend for weekend. Family is nice, yes?\",\n",
    "    \"The affect of the tsunami is incomprehensible\",\n",
    "    \"This cinnamom powder cut thru the grease.\",\n",
    "    \"I uh, likes pizza and also soda.\"\n",
    "]\n",
    "\n",
    "batch = gec_tokenizer(input_texts, truncation=True, padding='max_length', max_length=64, return_tensors=\"pt\")\n",
    "translated = gec_model.generate(**batch, num_beams=5, num_return_sequences=1, early_stopping=True)\n",
    "print(translated.shape)\n",
    "corrs = gec_tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "for inp, out in zip(input_texts, corrs):\n",
    "    print(f\"Orig: {inp}\\nCorr: {out}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8e922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
