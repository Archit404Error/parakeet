{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a2d36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val length: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archit/miniconda3/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "print(f\"Val length: {len(val_df)}\")\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, T5Tokenizer, T5ForConditionalGeneration\n",
    "gec_tokenizer = T5Tokenizer.from_pretrained(\"./gec_model_final\")\n",
    "gec_model = AutoModelForSeq2SeqLM.from_pretrained(\"./gec_model_final\")\n",
    "\n",
    "base_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af6579f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/500 [00:00<?, ?it/s]/Users/archit/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 500/500 [6:29:45<00:00, 46.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grammar: Parking, dining stopping and comfort rooms in the city Pasig Bayan.', 'For safty shide-change device id..', ': correct grammar: correct grammar: correct grammar: correct grammar: correct grammar: correct grammar', ':: Use correct spelling: Use correct grammar: Use correct grammar: Use correct grammar']\n",
      "rouge1\n",
      "AggregateScore(low=Score(precision=0.882235113606088, recall=0.6240742340506934, fmeasure=0.6990202902310695), mid=Score(precision=0.8857489672230149, recall=0.6311192927672602, fmeasure=0.7045775615411423), high=Score(precision=0.889071889710229, recall=0.6377160716600342, fmeasure=0.7098546001281772))\n",
      "rouge2\n",
      "AggregateScore(low=Score(precision=0.765615888656686, recall=0.5289343953742742, fmeasure=0.5953806220937231), mid=Score(precision=0.7716980404562785, recall=0.5360336563302136, fmeasure=0.6018972623265957), high=Score(precision=0.7776944923152849, recall=0.5431313650388109, fmeasure=0.6086543208887297))\n",
      "rougeL\n",
      "AggregateScore(low=Score(precision=0.8722794229518613, recall=0.6179779120788843, fmeasure=0.6918860240270989), mid=Score(precision=0.8762314519304326, recall=0.6250995442342578, fmeasure=0.6976581190613838), high=Score(precision=0.8799173338054785, recall=0.6318398847495986, fmeasure=0.7030408833787323))\n",
      "rougeLsum\n",
      "AggregateScore(low=Score(precision=0.872375076433013, recall=0.6177712668078436, fmeasure=0.6915953304944412), mid=Score(precision=0.8761380411612025, recall=0.6247735550486841, fmeasure=0.6972247522581265), high=Score(precision=0.8800797434000562, recall=0.6318851369622873, fmeasure=0.7028483435638172))\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "corrections = []\n",
    "base_corrections = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_df), BATCH_SIZE)):\n",
    "    sentence_batch = list(test_df[\"in\"])[i:i + BATCH_SIZE]\n",
    "    \n",
    "    batch_gec_tokens = gec_tokenizer(sentence_batch, padding='max_length', max_length=1024, return_tensors=\"pt\")\n",
    "    translated = gec_model.generate(**batch_gec_tokens,num_beams=5, num_return_sequences=1)\n",
    "    corrs = gec_tokenizer.batch_decode(translated, padding=\"longest\", skip_special_tokens=True)\n",
    "    corrections.extend(corrs)\n",
    "    \n",
    "    base_sentence_batch = [f\"correct grammar: {sentence}\" for sentence in sentence_batch]\n",
    "    \n",
    "    base_batch_tokens = base_tokenizer(base_sentence_batch, padding='max_length', max_length=1024, return_tensors=\"pt\")\n",
    "    base_translated = base_model.generate(**base_batch_tokens, num_beams=5, num_return_sequences=1)\n",
    "    base_corrs = base_tokenizer.batch_decode(base_translated, padding=\"longest\", skip_special_tokens=True)\n",
    "    base_corrections.extend(base_corrs)\n",
    "\n",
    "# Testing to see whether data was generated properly\n",
    "print(base_corrections[:4])\n",
    "    \n",
    "rouge_data = rouge_metric.compute(predictions=corrections, references=list(test_df[\"out\"]), use_stemmer=True)\n",
    "for key, val in rouge_data.items():\n",
    "    print(key)\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f043f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "to_serialize = {\n",
    "    \"gec_corrections\": corrections,\n",
    "    \"base_corrections\": base_corrections\n",
    "}\n",
    "\n",
    "# Serialize results to avoid having to re-generate later\n",
    "json.dump(to_serialize, open(\"corrections.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db37ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_predictions = json.load(open(\"corrections.json\", \"r\"))\n",
    "corrections = serialized_predictions[\"gec_corrections\"]\n",
    "base_corrections = serialized_predictions[\"base_corrections\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd25e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE DATA\n",
      "rouge1 : AggregateScore(low=Score(precision=0.3759902708640686, recall=0.2942409423274288, fmeasure=0.3147301813645697), mid=Score(precision=0.3873816837544118, recall=0.3037062288532276, fmeasure=0.3244145021798222), high=Score(precision=0.3980939048721737, recall=0.31279246728034243, fmeasure=0.3335493236608105))\n",
      "rouge2 : AggregateScore(low=Score(precision=0.3092195474815217, recall=0.23782304253551326, fmeasure=0.25463236940898076), mid=Score(precision=0.3199181645805176, recall=0.24659805018214329, fmeasure=0.26366938854437766), high=Score(precision=0.32963022051029, recall=0.25571714015385494, fmeasure=0.2724534498323195))\n",
      "rougeL : AggregateScore(low=Score(precision=0.36915925059673266, recall=0.28883047647642957, fmeasure=0.3086469656001121), mid=Score(precision=0.3807016655463552, recall=0.29927071826018903, fmeasure=0.31934303548720544), high=Score(precision=0.3934571908567431, recall=0.3096064068137392, fmeasure=0.3297915063773098))\n",
      "rougeLsum : AggregateScore(low=Score(precision=0.36919031156493914, recall=0.28918423954834854, fmeasure=0.30921352299821725), mid=Score(precision=0.380976326929864, recall=0.2993108000058961, fmeasure=0.3192090726942419), high=Score(precision=0.39241745559265245, recall=0.3088626966970805, fmeasure=0.32913181636884037))\n",
      "\n",
      "\n",
      "GEC DATA\n",
      "rouge1 : AggregateScore(low=Score(precision=0.882235113606088, recall=0.6240742340506934, fmeasure=0.6990202902310695), mid=Score(precision=0.8857489672230149, recall=0.6311192927672602, fmeasure=0.7045775615411423), high=Score(precision=0.889071889710229, recall=0.6377160716600342, fmeasure=0.7098546001281772))\n",
      "rouge2 : AggregateScore(low=Score(precision=0.765615888656686, recall=0.5289343953742742, fmeasure=0.5953806220937231), mid=Score(precision=0.7716980404562785, recall=0.5360336563302136, fmeasure=0.6018972623265957), high=Score(precision=0.7776944923152849, recall=0.5431313650388109, fmeasure=0.6086543208887297))\n",
      "rougeL : AggregateScore(low=Score(precision=0.8722794229518613, recall=0.6179779120788843, fmeasure=0.6918860240270989), mid=Score(precision=0.8762314519304326, recall=0.6250995442342578, fmeasure=0.6976581190613838), high=Score(precision=0.8799173338054785, recall=0.6318398847495986, fmeasure=0.7030408833787323))\n",
      "rougeLsum : AggregateScore(low=Score(precision=0.872375076433013, recall=0.6177712668078436, fmeasure=0.6915953304944412), mid=Score(precision=0.8761380411612025, recall=0.6247735550486841, fmeasure=0.6972247522581265), high=Score(precision=0.8800797434000562, recall=0.6318851369622873, fmeasure=0.7028483435638172))\n"
     ]
    }
   ],
   "source": [
    "rouge_gec_data = rouge_metric.compute(predictions=corrections, references=list(test_df[\"out\"]), use_stemmer=True)\n",
    "rouge_base_data = rouge_metric.compute(predictions=base_corrections, references=list(test_df[\"out\"]), use_stemmer=True)\n",
    "\n",
    "print(\"BASE DATA\")\n",
    "for key, val in rouge_base_data.items():\n",
    "    print(f\"{key} : {val}\")\n",
    "\n",
    "print(\"\\n\\nGEC DATA\")\n",
    "for key, val in rouge_gec_data.items():\n",
    "    print(f\"{key} : {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c11e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base Score: 22.162359254261354\n",
      "GEC Score: 33.24152826260671\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = load_metric(\"sacrebleu\")\n",
    "bleu_gec_data = bleu_metric.compute(predictions=[corrections], references=[list(test_df[\"out\"])])\n",
    "bleu_base_data = bleu_metric.compute(predictions=[base_corrections], references=[list(test_df[\"out\"])])\n",
    "\n",
    "print(f\"base Score: {bleu_base_data['score']}\")\n",
    "print(f\"GEC Score: {bleu_gec_data['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880d7b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8982a72483ca4a779e72c3bd40cf0407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129bdf95fb2940b6bc5eb24acf0b27ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: 559.820864630875\n",
      "GEC: 57.85254389276505\n"
     ]
    }
   ],
   "source": [
    "perplexity_metric = load_metric(\"perplexity\")\n",
    "perplexity_gec_score = perplexity_metric.compute(input_texts=corrections, model_id='gpt2')\n",
    "perplexity_base_score = perplexity_metric.compute(input_texts=list(filter(lambda x: x != \"\", base_corrections)), model_id='gpt2')\n",
    "print(f\"Base: {perplexity_base_score['mean_perplexity']}\")\n",
    "print(f\"GEC: {perplexity_gec_score['mean_perplexity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb92f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
